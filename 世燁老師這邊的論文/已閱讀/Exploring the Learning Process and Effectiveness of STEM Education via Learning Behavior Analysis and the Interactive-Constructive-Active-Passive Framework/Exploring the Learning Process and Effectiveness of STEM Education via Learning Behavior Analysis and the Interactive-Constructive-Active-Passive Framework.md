![upgit_20240516_1715852240.png](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2024/05/upgit_20240516_1715852240.png)
## 概要
本研究提出了一種結合深度學習和計算機視覺技術的新穎方法，利用 ICAP（互動-建構-主動-被動） 框架開發了 STEM 學習行為分析系統（SLBAS）。將 SLBAS 的輸出映射到學習過程中。以評估 STEM 教育中學習者的行為。
- SLBAS 能否有效評估學習者在 STEM 活動中的學習過程？
- 學習過程如何影響 STEM 活動中的學習效果？

### 針對STEM提出論述
- Hsiao等人（2022年）指出，STEM教育的特點是以學生為中心，培養學習者解決問題、協作和創造性思維的能力。
- Gao等人（2020）認為，在STEM教育中，只評價最終結果（如期末考試和期末項目）可能會減少學習者通過解決問題、跨學科整合和協作來建構知識的重要信息。

### 指出STEM學習過程
如何有效地了解學習者在STEM活動中的學習過程，也成為STEM教育評價中至關重要的一環。
- Gao 等人（2020）對 STEM 評估工具的系統回顧，文獻主要涉及自我報告法和觀察法。
- 自我報告法一般采用傳統的主觀測量方法，讓學習者通過問卷或訪談表達他們在學習過程中的動機信念、經驗和感受（Zimmerman，2008）。
- 觀察法通過對課堂數據進行編碼和標記，評估學習者的學習過程和效果（Harari 等人，2017；Lathia等人，2013）。
- 我報告法和觀察法的局限性：
	- 自我報告容易受到主觀意識、受試者記憶局限和社會期望的幹擾（Baumeister 等人，2007 年；Paulhus & Vazire，2007 年）
	- D'Mello 等人（2017 年）指出，觀察法的編碼和標注耗時耗力，而且無法在某些學習環境（如學習者家中）中執行。

## 文獻回顧 基於深度學習的人類行為識別

大致提一下：
- 人類行為識別因其在人機交互、體育和醫療保健等領域的廣泛應用，在過去幾年里受到了廣泛關注（Khan 等人，2020；Majd & Safabakhsh，2020）。
- 常見的人類行為識別方法可分為：
- 基於圖像的人類行為識別（Dai 等人，2020；Jaouedi 等人，2020；Khan 等人，2020a），具有最高的識別精度和易於部署的特點，因此在實踐中得到了最多的研究關注
- 基於可穿戴設備的人類行為識別（Demrozi 等人，2020；Nweke 等人，2019）
- 基於無線網絡設備的人類行為識別（Cui 等人，2021；Liu 等人，2020）。


### kinect感應器（蝦皮的東西）
應用：台階測量和健康監測（Al-Naji 等人，2017 年）。
缺點：
- 三維人體關鍵點的提取需要特定的傳感器和深度攝像頭，關節重疊容易導致誤差（Lin 等人，2021 年）。
### OpenPose 系統
- Cao et al.(2019) 提出了 OpenPose 系統，該系統可從二維 RGB 圖像中快速提取人體關鍵點，並采用部件親和場（PAF）連接人體關鍵點，完成行為識別。與微軟 Kinect 相比，OpenPose 只需要市面上的攝像頭就能完成關鍵點提取，並采用卷積神經網絡（CNN）解決重疊問題（Cao et al., 2019）
- Nakai 等人（2018 年）利用 OpenPose 提取關鍵點，並以此為基礎評估籃球比賽中的投籃動作，預測這種投籃行為命中籃筐的可能性。
- Yan 等人（2020）將 OpenPose 和卡爾曼濾波器結合起來，跟蹤病人在康覆過程中的動作準確性，從而了解他們的狀態。
- Qwu 等人（2021 年）在 OpenPose 的基礎上提出了 ROpenPose，用於檢測宇航員在失重環境下的運動和操作行為。
- Lin 等人（2021 年）在一款羽毛球教學應用程序中利用 OpenPose 進行實時姿態識別，幫助學生提高羽毛球技能和學習效率。
- 本文作者：由於 STEM 教育的多樣性和動態性（Brown et al. 2011）基於 OpenPose 的系統來識別學生在 STEM 活動中的行為，並利用這些信息來評估學習過程。

## 文獻回顧 
ICAP框架

由 Chi 和 Wylie（2014 年）提出，將學習過程分為四種不同的模式。
- 被動式： 學習者被動地接受教學材料中的信息，不主動做任何與學習有關的事情。例如，學習者認真聽講，不做筆記或記錄等其他事情。
- 主動式：學習者積極表現出明顯的行為或身體操作。例如，學習者在觀看學習視頻時可能會暫停、快進或倒退，他們可能會突出顯示學習內容並做筆記，或者他們可能會通過手勢操作學習材料來解決手頭的問題。
- 建設性： 這是指學習者在學習材料所提供的內容之外，產生或制作額外的外部化輸出或產品的行為。例如，學習者通過組裝和操作機器人來建構編程知識，繪制概念圖來理解自己建構知識的過程。
-  ﻿互動性： 互動行為的可操作性需要兩個標準，包括雙方的發言必須以建設性為主，必須有足夠程度的輪流發言（Chi 和 Wylie，2014 年）。Chi 和 Wylie（2014）指出，雙方都必須參與建設性行為，這樣這些討論才有意義。此外，當互動中有足夠的輪流時，雙方更容易整合彼此對該領域的理解並調整自己的認知狀態。只要符合這些標準，互動者就不一定是同伴；例如，教師、父母或機器人也可以是互動者。

應用：
- ICAP 框架將學習者的學習過程視為一個漸進的過程（Chi & Wylie, 2014）。學習者通過使用學習材料來達到越來越高的水平，逐漸加深他們的認知狀態和對課程的參與（Chi & Wylie, 2014; Sailer et al.） 鑒於 ICAP 對學習者參與度的描述，許多研究在評估學習過程時將該框架作為一項指標（Hsiao et al.）。Zhang 等人（2016）根據 ICAP 框架定義了學習者在線學習期間的行為指標，幫助研究人員更好地理解在線學習期間的參與度和習慣。
- Atapattu等人（2019）利用自然語言處理（NLP）中的詞嵌入（word2vec）開發了一個自動化系統，並整合了ICAP框架作為評估學習過程的指標。
- Rakovié 等人（2020 年）將該框架用於分析論壇中的學習者信息，幫助教師理解知識建構過程，並作為預測學習者考試和辯論效果的指標。
- 本文作者：開發了一個基於 ICAP 框架的系統，以幫助研究人員和教師更好地理解 STEM 教育中的學習過程，並作為 STEM 教育中教育評估的基礎。


## STEM 教育中的評估
開頭：在 Gao 等人（2020）的系統綜述中，自我報告法和觀察法是評估 STEM 教育中學習者學習效果和學習過程的主要方法。
### 自我報告法：
- 學習者通過傳統的主觀測量方法，以問卷或訪談的形式表達自己在學習過程中的動機信念、經驗和感受（Zimmerman，2008）。
- 例如，Sahin 和 Yilmaz（2020 年）使用關於對科學和增強現實（AR）的態度的調查問卷來了解在 STEM 活動中使用 AR 技術後學習者對科學和 AR 的態度的變化。
- Ekatushabe 等人（2021 年）使用自我報告問卷來評估參與 STEM 活動時教師的自主支持、自我效能感和厭煩程度。
- 缺點：一些研究表明，自我報告容易受到主觀意識、受試者記憶局限以及社會期望的幹擾（Baumeister 等人，2007 年；Paulhus & Vazire，2007 年）。

### 觀察法：
- 以相對客觀的方式評估課堂學習效果（Harari 等人，2017 年；Lathia 等人，2013 年）。
- 觀察法通過對傳感器或儀器收集到的數據進行編碼和標記，客觀地測量學習過程和學習效果（Harari 等人，2017；Lathia 等人，2013）。
- 例如，（Chen等人，2020）開發了四個適合動手學習的行為指標，並邀請兩位專家通過開發的指標對動手虛擬現實（VR）活動視頻進行編碼，幫助研究人員全面了解學習過程。
- Sun等人（2021）邀請兩位專家對不插電STEM活動視頻中的學習者行為進行編碼，發現傳統課堂課程與不插電STEM活動中學習過程和學習行為的差異。
- 缺點：D'Mello 等人（2017）發現，觀察法在編碼和標注方面耗時耗力，且無法在學習者家中等學習環境中進行。
### 加入深度學習技術

- 隨著技術的飛速發展，越來越多的研究利用基於深度學習的識別來減少觀察所需的時間和人力（Cojocea & Rebedea, 2022; He et al.） 例如，
- Zhang 等人（2020）利用攝像頭記錄學習者的面部表情，作為判斷學習者在線學習參與度的依據；
- Liu 等人（2019）利用教師手上的關鍵點來識別手勢，並探討利用手勢吸引學習者注意力對學習效果的影響。
- 梁等人（2019）認為，隨著智能設備的普及，傳統教育模式正日益受到挑戰。因此，他們通過智能手表中的傳感器分析學習者的身體狀況和交互情況，並在診斷後使用自適應反饋
- Kim 等人（2021 年）利用計算機視覺技術分析每個關鍵點的移動，探討芳香療法是否會影響學習者的學習效果。
- 本為作者：
	- 盡管最近有許多研究將深度學習技術用於教育評估，但很少有研究專注於建立系統模型來探索以學生為中心的 STEM 教育。
	- 為了解決自我報告和觀察方法的局限性，填補目前 STEM 教育評估的空白，我們將深度學習和計算機視覺結合起來，識別雙手在學習材料上的動作，幫助研究人員評估 STEM 活動中的學習行為。然後，我們將這種學習行為映射到 ICAP 框架，作為客觀評估學習過程的基礎。


## Method
我們提出了七個步驟來了解學習者的學習過程，並確定 STEM 活動中學習過程與學習效果之間的關系。
![upgit_20240516_1715843134.png](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2024/05/upgit_20240516_1715843134.png)

### 步驟一：收集模擬數據
遷移學習：
COCO data set： https://www.youtube.com/watch?v=TLvdlDgZ3G0



### 步驟二：構建 STEM 學習行為分析系統（SLBAS）
回顧 Hofstein 和 Lunetta（1982 年）對學習過程的定義，包括學習者與學習材料之間的互動。
辨識手部：
![upgit_20240516_1715843558.png](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2024/05/upgit_20240516_1715843558.png)

- 使用 OpenPose（Martin Paez 等人，2019 年）捕捉學習者的手部關鍵點
- YOLOv4（Bochkovskiy 等人，2020 年）檢測學習材料的位置，逐幀處理 STEM 活動視頻。
- SLBAS 通過結合手部關鍵點和學習材料的位置來分析學習過程，從而識別 STEM 活動中的學習行為。

### 步驟三：從 ICAP 框架定義 STEM 學習過程指標
本研究基於學習材料的手部動作來定義學習行為：當學習者接觸學習材料時，其行為就會被識別出來。

| 指標Indicator | 定義                                                  | 行為                                                                       |
| ----------- | --------------------------------------------------- | ------------------------------------------------------------------------ |
| 被動式         | 學習者被動地從教學材料中接收資訊，不主動做任何與學習有關的事情（Chi&Wylie，2014）     | 學習者的手不要觸摸任何與STEM研討會相關的物體。                                                |
| 主動式         | 學習者積極採用可見的行為或身體操作（Chi&Wylie，2014）                   | - 學習者用鉛筆做筆記，記錄他們正在學習的內容<br>- 學習者主動操作包含學習材料的平板電腦來解決他們的問題，而不是被動地從老師那裡接受知識。 |
| 建設性         | 學習者採用的行為是，他們產生或產生超出學習材料提供的額外外化輸出或產品（Chi&Wylie，2014） | - 學習者使用鍵盤和滑鼠操作筆記型電腦以完成程式設計<br>- 學習者拿起Arduino元件並手工組裝以解決任務                 |
| 互動性         | 學習者通過在對話中提出問題和回應他人來構建自己的知識（Chi&Wylie，2014）          | 學習者在構建知識時遇到問題時向助教提問                                                      |


![upgit_20240516_1715844765.png](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2024/05/upgit_20240516_1715844765.png)


### 步驟四：使用模擬數據評估 SLBAS 的性能

- 為了解決第一個研究問題，本研究通過模擬數據來驗證 SLBAS 的性能，以確保 SLBAS 的輸出足夠有效和高效，足以取代觀察方法。
- 為了進行驗證，我們使用了準確率、召回率、精確率和混淆矩陣來評估每種學習行為和學習過程的性能。
- 為了確認專家編碼是否能替代 SLBAS 輸出，我們邀請了兩位專家對模擬數據中的相同視頻進行編碼。
- 我們使用評分者間的可靠性來測量專家的編碼和 SLBAS 的輸出。高卡帕值表明，SLBAS 與耗時耗力的人工編碼具有相同的性能。


### 步驟五：該系統使用於Arduino的避障車

![upgit_20240516_1715847862.png](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2024/05/upgit_20240516_1715847862.png)


### 步驟六：在STEM研討會期間使用SLBAS識別學習過程
- 所有參與者在研討會期間的行為和學習過程都由商用網路攝像頭以 1920 x 1080 解析度和 20 fps 記錄下來。攝像機的拍攝角度如圖5所示。請注意，參與者的手和學習材料應盡可能在螢幕上。工作坊結束后，將收集到的視頻輸入SLBAS，以識別學習行為和學習過程，以用於後續分析。

### 步驟七：資料分析
- 學習效能測試來檢查參與者在構建避障車時的程式設計技能和理解能力。（20 道單選題）
- Cronbach's a 值為 .71，足以獲得可靠的結果（Nunnally，1978）。
- 利用Pearson相關係數確認學習過程與學習效能的關係
- 根據他們的學習效果測試成績分為兩組（高、低學習效能組），進行獨立t檢驗，計算Pearson相關係數，確定高學習效能組和低學習效能組各學習過程指標的差異。

## 研究結果：
### SLBAS是否有效地協助教師和研究人員評估學習者在STEM活動中的學習過程？

混淆矩陣 評估了SLBAS的性能（和準確度、精密度和召回率）
- 建設性指標很容易與被動指標 （n = 14） 混淆。這是因為學習者經常將手放在用於完成任務的學習材料附近，即使他們什麼都不做。
- 被動指標很容易與主動指標（n = 12）和其他指標（n = 11）混淆。由於視頻捕獲速率僅為 20 fps，因此當學習者移動時，小型學習材料（例如筆和智能手機）很容易模糊，從而阻礙了對小物體的檢測。由於小物體無法識別，因此小物體定義的指標很容易混淆。

測試SLBAS性能：Cohen's kappa（可以使用SPSS計算）
參考教學：https://www.yongxi-stat.com/cohens-kappa/
我們計算了Cohen的kappa來比較SLBAS輸出和專家編碼。兩名專家和SLBAS從模擬數據中對三個一分鐘的視頻進行了編碼，得出專家A和專家B之間的Cohen's kappa為0.74，專家A和SLBAS為0.71，專家B和SLBAS為0.72，超過了0.70（Landis&Koch，1977）。這表明評分者間的信度足夠高;也就是說，專家的編碼與SLBAS的編碼之間沒有區別。因此，SLBAS可以用來代替專家在觀察方法中進行編碼。此外，SLBAS不僅在觀察方法中表現出與專家編碼相同的準確性，而且還可以自動編碼，從而大大減少了時間和人力成本。



### 在STEM活動中學習過程如何影響學習效果？

第一步，計算Pearson相關性，確認學習過程中各指標與學習效能的關係。
![upgit_20240516_1715848831.png](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2024/05/upgit_20240516_1715848831.png)


第二步，為探究學習效果，將學習者分為兩組（高、低學習效能組）。得分最高的 50% 學習者被分配到高學習效能組。

第三步，為了確定高學習效能組和低學習效能組之間的差異，進行了獨立的t檢驗
![upgit_20240516_1715849001.png](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2024/05/upgit_20240516_1715849001.png)

在被動 （t = 2.655， p < .05）、建設性 （t= -3.635， p <.01）、和互動 （t = 10.768， p < 001）中，高效組有效性表現出更高的建設性和互動性指標，而被動性指標更低。

高效組的學習指標相關係數
![upgit_20240516_1715849158.png](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2024/05/upgit_20240516_1715849158.png)

低效組的學習指標相關係數
![upgit_20240516_1715849146.png](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2024/05/upgit_20240516_1715849146.png)




